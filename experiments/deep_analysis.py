"""
æ·±åº¦è¿ç§»åˆ†æ - æ‰¾åˆ°è¿ç§»å·®çš„æ ¹æœ¬åŸå› 

åˆ†æç»´åº¦:
1. ä¸åŒæ¶æ„æ—ä¹‹é—´çš„æ¢¯åº¦å·®å¼‚
2. æ¨¡å‹å®¹é‡ä¸è¿ç§»çš„å…³ç³»
3. æ‰°åŠ¨å¹…åº¦ (epsilon) å¯¹è¿ç§»çš„å½±å“
4. å±‚çº§ç‰¹å¾ç›¸ä¼¼æ€§åˆ†æ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import numpy as np
import json
from datetime import datetime
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from hub_models import get_hub_model
from negative_strategies import get_strategy
from ff_attack import FFAttack


def get_model_architecture_info():
    """æ¨¡å‹æ¶æ„ä¿¡æ¯"""
    return {
        'resnet56': {'family': 'ResNet', 'params': '0.86M', 'depth': 56},
        'resnet20': {'family': 'ResNet', 'params': '0.27M', 'depth': 20},
        'resnet44': {'family': 'ResNet', 'params': '0.66M', 'depth': 44},
        'vgg16_bn': {'family': 'VGG', 'params': '14.7M', 'depth': 16},
        'vgg13_bn': {'family': 'VGG', 'params': '9.4M', 'depth': 13},
        'mobilenetv2_x1_0': {'family': 'MobileNet', 'params': '2.2M', 'depth': 52},
        'shufflenetv2_x1_0': {'family': 'ShuffleNet', 'params': '1.3M', 'depth': 50},
        'repvgg_a0': {'family': 'RepVGG', 'params': '8.3M', 'depth': 22},
    }


def analyze_cross_architecture_transfer(device):
    """åˆ†æä¸åŒæ¶æ„æ—ä¹‹é—´çš„è¿ç§»"""
    print("\n" + "="*70)
    print("CROSS-ARCHITECTURE TRANSFER ANALYSIS")
    print("="*70)
    
    # åŠ è½½å¤šä¸ªæºæ¨¡å‹
    source_models = {
        'resnet56': get_hub_model('resnet56', pretrained=True, device=device),
        'vgg16_bn': get_hub_model('vgg16_bn', pretrained=True, device=device),
        'mobilenetv2_x1_0': get_hub_model('mobilenetv2_x1_0', pretrained=True, device=device),
    }
    
    target_models = {
        'resnet20': get_hub_model('resnet20', pretrained=True, device=device),
        'vgg13_bn': get_hub_model('vgg13_bn', pretrained=True, device=device),
        'shufflenetv2_x1_0': get_hub_model('shufflenetv2_x1_0', pretrained=True, device=device),
        'repvgg_a0': get_hub_model('repvgg_a0', pretrained=True, device=device),
    }
    
    for m in source_models.values():
        m.eval()
    for m in target_models.values():
        m.eval()
    
    # åŠ è½½æ•°æ®
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    subset = Subset(testset, range(300))
    loader = DataLoader(subset, batch_size=50, shuffle=False)
    
    arch_info = get_model_architecture_info()
    results = {}
    
    for source_name, source_model in source_models.items():
        print(f"\n--- Source: {source_name} ({arch_info[source_name]['family']}) ---")
        
        # åˆ›å»ºæ”»å‡»
        attacker = FFAttack(source_model, eps=8/255, alpha=2/255, steps=10, device=device)
        
        # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
        all_adv = []
        all_labels = []
        all_targets = []
        
        strategy = get_strategy('most_confusing')
        
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            
            with torch.no_grad():
                logits = source_model(images)
            targets = torch.tensor([strategy.get_target(logits[i], labels[i].item()) 
                                   for i in range(len(labels))], device=device)
            
            adv_images = attacker(images, targets)
            all_adv.append(adv_images)
            all_labels.append(labels)
            all_targets.append(targets)
        
        all_adv = torch.cat(all_adv)
        all_labels = torch.cat(all_labels)
        all_targets = torch.cat(all_targets)
        
        results[source_name] = {}
        
        # æµ‹è¯•è¿ç§»åˆ°å„ç›®æ ‡æ¨¡å‹
        for target_name, target_model in target_models.items():
            with torch.no_grad():
                target_out = target_model(all_adv)
                target_pred = target_out.argmax(dim=1)
                
                # ç›®æ ‡æˆåŠŸç‡
                target_success = (target_pred == all_targets).float().mean().item()
                # è¯¯åˆ†ç±»ç‡
                misclass = (target_pred != all_labels).float().mean().item()
            
            source_family = arch_info[source_name]['family']
            target_family = arch_info[target_name]['family']
            same_family = source_family == target_family
            
            results[source_name][target_name] = {
                'target_success': target_success,
                'misclass': misclass,
                'same_family': same_family,
            }
            
            marker = "âœ“" if same_family else "âœ—"
            print(f"  â†’ {target_name} ({target_family}) [{marker} same family]: "
                  f"Target={target_success*100:.1f}%, Misclass={misclass*100:.1f}%")
    
    return results


def analyze_epsilon_impact(device):
    """åˆ†æä¸åŒæ‰°åŠ¨å¹…åº¦å¯¹è¿ç§»çš„å½±å“"""
    print("\n" + "="*70)
    print("EPSILON IMPACT ANALYSIS")
    print("="*70)
    
    source_model = get_hub_model('resnet56', pretrained=True, device=device)
    source_model.eval()
    
    target_models = {
        'vgg16_bn': get_hub_model('vgg16_bn', pretrained=True, device=device),
        'mobilenetv2_x1_0': get_hub_model('mobilenetv2_x1_0', pretrained=True, device=device),
    }
    for m in target_models.values():
        m.eval()
    
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    subset = Subset(testset, range(300))
    loader = DataLoader(subset, batch_size=100, shuffle=False)
    
    epsilons = [2/255, 4/255, 8/255, 16/255, 32/255]
    results = {eps: {} for eps in epsilons}
    
    strategy = get_strategy('most_confusing')
    
    for eps in epsilons:
        print(f"\nEpsilon = {eps*255:.0f}/255")
        
        attacker = FFAttack(source_model, eps=eps, alpha=eps/4, steps=10, device=device)
        
        all_adv = []
        all_labels = []
        all_targets = []
        
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            
            with torch.no_grad():
                logits = source_model(images)
            targets = torch.tensor([strategy.get_target(logits[i], labels[i].item()) 
                                   for i in range(len(labels))], device=device)
            
            adv_images = attacker(images, targets)
            all_adv.append(adv_images)
            all_labels.append(labels)
            all_targets.append(targets)
        
        all_adv = torch.cat(all_adv)
        all_labels = torch.cat(all_labels)
        all_targets = torch.cat(all_targets)
        
        # ç™½ç›’
        with torch.no_grad():
            source_out = source_model(all_adv)
            source_pred = source_out.argmax(dim=1)
            whitebox = (source_pred == all_targets).float().mean().item()
        
        results[eps]['whitebox'] = whitebox
        print(f"  Whitebox: {whitebox*100:.1f}%")
        
        # è¿ç§»
        for target_name, target_model in target_models.items():
            with torch.no_grad():
                target_out = target_model(all_adv)
                target_pred = target_out.argmax(dim=1)
                transfer = (target_pred == all_targets).float().mean().item()
            
            results[eps][target_name] = transfer
            print(f"  â†’ {target_name}: {transfer*100:.1f}%")
    
    # æ€»ç»“
    print("\n--- Summary ---")
    print(f"{'Epsilon':<10} {'Whitebox':<12} {'Avg Transfer':<12}")
    for eps in epsilons:
        whitebox = results[eps]['whitebox']
        transfers = [v for k, v in results[eps].items() if k != 'whitebox']
        avg_transfer = np.mean(transfers)
        print(f"{eps*255:.0f}/255     {whitebox*100:>6.1f}%      {avg_transfer*100:>6.1f}%")
    
    return results


def analyze_gradient_divergence_by_layer(device):
    """åˆ†æä¸åŒå±‚çš„æ¢¯åº¦å‘æ•£ç¨‹åº¦"""
    print("\n" + "="*70)
    print("LAYER-WISE GRADIENT DIVERGENCE ANALYSIS")
    print("="*70)
    
    source_model = get_hub_model('resnet56', pretrained=True, device=device)
    target_model = get_hub_model('vgg16_bn', pretrained=True, device=device)
    source_model.eval()
    target_model.eval()
    
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
    subset = Subset(testset, range(50))
    loader = DataLoader(subset, batch_size=50, shuffle=False)
    
    images, labels = next(iter(loader))
    images, labels = images.to(device), labels.to(device)
    
    # æ”¶é›†å„å±‚æ¢¯åº¦
    source_grads = {}
    target_grads = {}
    
    def make_hook(storage, name):
        def hook(module, grad_input, grad_output):
            if grad_output[0] is not None:
                storage[name] = grad_output[0].detach()
        return hook
    
    # æ³¨å†Œ hooks
    source_handles = []
    target_handles = []
    
    layer_idx = 0
    for name, module in source_model.named_modules():
        if isinstance(module, nn.Conv2d):
            handle = module.register_full_backward_hook(make_hook(source_grads, f'conv_{layer_idx}'))
            source_handles.append(handle)
            layer_idx += 1
    
    layer_idx = 0
    for name, module in target_model.named_modules():
        if isinstance(module, nn.Conv2d):
            handle = module.register_full_backward_hook(make_hook(target_grads, f'conv_{layer_idx}'))
            target_handles.append(handle)
            layer_idx += 1
    
    # å‰å‘å’Œåå‘ä¼ æ’­
    images.requires_grad = True
    
    source_out = source_model(images)
    source_loss = F.cross_entropy(source_out, labels)
    source_loss.backward()
    
    images.requires_grad = True
    target_out = target_model(images)
    target_loss = F.cross_entropy(target_out, labels)
    target_loss.backward()
    
    # æ¸…ç† hooks
    for h in source_handles + target_handles:
        h.remove()
    
    # åˆ†æ
    print("\nSource model has", len(source_grads), "conv layers")
    print("Target model has", len(target_grads), "conv layers")
    
    # æ¯”è¾ƒå‰å‡ å±‚å’Œåå‡ å±‚çš„è¾“å…¥æ¢¯åº¦
    print("\nè¾“å…¥å±‚æ¢¯åº¦ç›¸ä¼¼åº¦ï¼ˆè¿™æ˜¯å†³å®šè¿ç§»çš„å…³é”®ï¼‰:")
    images.requires_grad = True
    
    source_out = source_model(images)
    source_loss = F.cross_entropy(source_out, labels)
    source_input_grad = torch.autograd.grad(source_loss, images, create_graph=False)[0]
    
    images.requires_grad = True
    target_out = target_model(images)
    target_loss = F.cross_entropy(target_out, labels)
    target_input_grad = torch.autograd.grad(target_loss, images, create_graph=False)[0]
    
    # æŒ‰é€šé“åˆ†æ
    for c in range(3):
        source_c = source_input_grad[:, c].flatten()
        target_c = target_input_grad[:, c].flatten()
        cos_sim = F.cosine_similarity(source_c.unsqueeze(0), target_c.unsqueeze(0)).item()
        print(f"  Channel {c} (RGB[{c}]): cosine similarity = {cos_sim:.4f}")
    
    # æ€»ä½“
    source_flat = source_input_grad.flatten()
    target_flat = target_input_grad.flatten()
    total_cos = F.cosine_similarity(source_flat.unsqueeze(0), target_flat.unsqueeze(0)).item()
    print(f"  Overall: cosine similarity = {total_cos:.4f}")
    
    return {
        'source_conv_layers': len(source_grads),
        'target_conv_layers': len(target_grads),
        'input_grad_similarity': total_cos,
    }


def main():
    device = torch.device('cuda' if torch.cuda.is_available() 
                         else 'mps' if torch.backends.mps.is_available() 
                         else 'cpu')
    print(f"Device: {device}")
    
    results = {}
    
    # 1. è·¨æ¶æ„è¿ç§»åˆ†æ
    results['cross_architecture'] = analyze_cross_architecture_transfer(device)
    
    # 2. Epsilon å½±å“åˆ†æ
    results['epsilon_impact'] = analyze_epsilon_impact(device)
    
    # 3. å±‚çº§æ¢¯åº¦åˆ†æ
    results['gradient_divergence'] = analyze_gradient_divergence_by_layer(device)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'results/deep_analysis_{timestamp}.json'
    
    # è½¬æ¢ numpy ç±»å‹
    def convert(obj):
        if isinstance(obj, np.floating):
            return float(obj)
        if isinstance(obj, np.integer):
            return int(obj)
        if isinstance(obj, dict):
            return {k: convert(v) for k, v in obj.items()}
        return obj
    
    results = convert(results)
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ“ Results saved to {output_file}")
    
    # æœ€ç»ˆç»“è®º
    print("\n" + "="*70)
    print("FINAL CONCLUSIONS")
    print("="*70)
    
    print("""
ğŸ” ä¸ºä»€ä¹ˆè¿ç§»ç‡ä½ï¼Ÿ

1. **æ¶æ„å·®å¼‚å¯¼è‡´æ¢¯åº¦æ–¹å‘å®Œå…¨ä¸åŒ**
   - ä¸åŒæ¶æ„çš„æ¨¡å‹å­¦ä¹ äº†å®Œå…¨ä¸åŒçš„ç‰¹å¾è¡¨ç¤º
   - ResNet çš„æ®‹å·®è¿æ¥ vs VGG çš„çº¯å †å  â†’ æ¢¯åº¦æµå®Œå…¨ä¸åŒ
   - æ¢¯åº¦ä½™å¼¦ç›¸ä¼¼åº¦ ~0.09ï¼Œå‡ ä¹æ­£äº¤ï¼

2. **å†³ç­–è¾¹ç•Œå‡ ä½•ä¸åŒ**
   - æ¯ä¸ªæ¨¡å‹çš„å†³ç­–è¾¹ç•Œåœ¨é«˜ç»´ç©ºé—´çš„å½¢çŠ¶å®Œå…¨ä¸åŒ
   - å¯¹ ResNet æœ‰æ•ˆçš„æ‰°åŠ¨æ–¹å‘å¯èƒ½ä¸ VGG çš„è¾¹ç•Œå¹³è¡Œ

3. **FreezeOut å¯èƒ½åŠ å‰§äº†è¿‡æ‹Ÿåˆåˆ°æºæ¨¡å‹**
   - æ¸è¿›å†»ç»“å¯¼è‡´æ”»å‡»è¿‡åº¦é€‚é…æºæ¨¡å‹çš„ç‰¹å®šå±‚
   - è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆ FreezeOut ç™½ç›’å¼ºä½†è¿ç§»å¼±

ğŸ“ˆ ä¸ºä»€ä¹ˆ MI-DI-FGSM æ›´å¥½ï¼Ÿ

   - Momentum: å¹³æ»‘æ¢¯åº¦ï¼Œå‡å°‘å™ªå£°
   - Input Diversity: å¢åŠ æ¢¯åº¦å¤šæ ·æ€§ï¼Œä¸è¿‡æ‹Ÿåˆå•ä¸€è¾“å…¥
   - è¿™ä¸¤è€…éƒ½åœ¨"æ­£åˆ™åŒ–"æ”»å‡»ï¼Œä½¿å…¶æ›´é€šç”¨

ğŸ’¡ æ”¹è¿›æ–¹å‘ï¼š

   1. **é›†æˆæ”»å‡»**: åŒæ—¶å¯¹å¤šä¸ªæ¨¡å‹ä¼˜åŒ–
   2. **ç‰¹å¾å±‚æ”»å‡»**: æ”»å‡»ä¸­é—´ç‰¹å¾è€Œéè¾“å‡º
   3. **å…ƒå­¦ä¹ **: å­¦ä¹ å¯è¿ç§»çš„æ”»å‡»æ¨¡å¼
""")


if __name__ == '__main__':
    main()
